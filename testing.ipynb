{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(\"/workspace/pretrained_models/fcpe_c_v001.pt\", map_location=\"cpu\")\n",
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.cfm.dit import DiT\n",
    "import torch\n",
    "\n",
    "dit = DiT(\n",
    "    in_channels=128 * 2,\n",
    "    hidden_channels=192,\n",
    "    out_channels=128,\n",
    "    filter_channels=192 * 4,\n",
    "    dropout=0.05,\n",
    "    n_layers=8,\n",
    "    n_heads=4,\n",
    "    dim_head=64,\n",
    "    kernel_size=3,\n",
    "    utt_emb_dim=384,\n",
    "    use_skip_connections=False,\n",
    ")\n",
    "\n",
    "# print dit parameter count\n",
    "print(f\"DiT parameter count: {sum(p.numel() for p in dit.parameters())}\")\n",
    "\n",
    "x = torch.randn(1, 128, 384)\n",
    "x_mask = torch.ones(1, 1, 384)\n",
    "mu = torch.randn(1, 128, 384)\n",
    "t = torch.Tensor([0.2])\n",
    "spks = torch.randn(1, 384)\n",
    "cond = torch.randn(1, 192, 32)\n",
    "cond_mask = torch.ones(1, 1, 32)\n",
    "\n",
    "dit(x, x_mask, mu, t, spks).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import SynthesizerTrn\n",
    "\n",
    "vc_model = SynthesizerTrn(\n",
    "    spec_channels=128,\n",
    "    hidden_channels=192,\n",
    "    filter_channels=768,\n",
    "    n_heads=4,\n",
    "    dim_head=64,\n",
    "    n_layers=6,\n",
    "    kernel_size=3,\n",
    "    p_dropout=0.1,\n",
    "    speaker_embedding=384,\n",
    "    n_speakers=10,\n",
    "    ssl_dim=768,\n",
    ")\n",
    "\n",
    "c = torch.randn(1, 768, 565)\n",
    "c_lengths = torch.LongTensor([565])\n",
    "energy = torch.randn(1, 1, 565)\n",
    "spec = torch.randn(1, 128, 565)\n",
    "f0 = torch.randn(1, 1, 565)\n",
    "uv = torch.ones(1, 565)\n",
    "g = torch.randn(1, 384)\n",
    "ppg = torch.randint(0, 40, (1, 124))\n",
    "ppg_lengths = torch.LongTensor([124])\n",
    "ppg_dur = torch.randint(0, 10, (1, 124)).float()\n",
    "\n",
    "\n",
    "# print dit parameter count\n",
    "print(f\"VC model parameter count: {sum(p.numel() for p in vc_model.parameters())}\")\n",
    "\n",
    "# (prior_loss, diff_loss, f0_pred, lf0)\n",
    "vc_model(\n",
    "    c=c,\n",
    "    f0=f0,\n",
    "    uv=uv,\n",
    "    spec=spec,\n",
    "    energy=energy,\n",
    "    c_lengths=c_lengths,\n",
    "    ppg=ppg,\n",
    "    ppg_lengths=ppg_lengths,\n",
    "    ppg_dur=ppg_dur,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = vc_model.infer(\n",
    "    c=c,\n",
    "    spec=spec,\n",
    "    f0=f0,\n",
    "    uv=uv,\n",
    "    energy=energy,\n",
    "    ppg=ppg,\n",
    "    ppg_lengths=ppg_lengths,\n",
    "    ppg_dur=ppg_dur,\n",
    ")\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_utils import TextAudioCollate, TextAudioSpeakerLoader\n",
    "import utils\n",
    "\n",
    "hps = utils.get_hparams_from_file(\"/home/cfm-vc/configs/config_ppgs.json\")\n",
    "\n",
    "collate_fn = TextAudioCollate()\n",
    "eval_dataset = TextAudioSpeakerLoader(hps.data.training_files, hps, all_in_mem=False)\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(eval_loader):\n",
    "    c, f0, spec, lengths, uv, energy, sid, ppg, ppg_lengths, ppg_dur = batch\n",
    "\n",
    "    (\n",
    "        prior_loss,\n",
    "        diff_loss,\n",
    "        loss_dur,\n",
    "        f0_pred,\n",
    "        lf0,\n",
    "        energy_pred,\n",
    "        speaker_logits,\n",
    "    ) = vc_model(\n",
    "        c=c,\n",
    "        f0=f0,\n",
    "        uv=uv,\n",
    "        spec=spec,\n",
    "        energy=energy,\n",
    "        c_lengths=c_lengths,\n",
    "        ppg=ppg,\n",
    "        ppg_lengths=ppg_lengths,\n",
    "        ppg_dur=ppg_dur,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, cond, cond_mask = vc_model.compute_conditional_latent([spec], [c_lengths])\n",
    "g.shape, cond.shape, cond_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.reference_encoder import MelStyleEncoder\n",
    "import torch\n",
    "\n",
    "\n",
    "mel_encoder = MelStyleEncoder(\n",
    "    in_channels=128,\n",
    "    hidden_channels=256,\n",
    "    utt_channels=512,\n",
    "    kernel_size=5,\n",
    "    p_dropout=0.1,\n",
    "    n_heads=4,\n",
    "    dim_head=64,\n",
    ")\n",
    "\n",
    "# print dit parameter count\n",
    "print(f\"VC model parameter count: {sum(p.numel() for p in mel_encoder.parameters())}\")\n",
    "\n",
    "spec = torch.randn(1, 128, 56)\n",
    "spec_mask = torch.ones(1, 1, 56)\n",
    "\n",
    "\n",
    "g, cond, cond_mask = mel_encoder(spec, spec_mask)\n",
    "print(g, cond.shape, cond_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "train_all = [\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_borderlands2_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_baldursgate3_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_worldofwarcraft_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_mario_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/de_gametts_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/pl_archolos_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/de_borderlands2_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_warcraft_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_sqnarrator_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_emotional_train_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/de_emotional_train_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/ru_witcher3_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_witcher3_skyrim_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_fallout4_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_naruto_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/de_kcd_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/pl_witcher3_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/de_diablo4_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_diablo4_xphone.csv\",\n",
    "    # \"/workspace/metadata/filelists/xphoneBERT/fr_diablo4_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/pl_diablo4_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/ru_diablo4_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/ru_skyrim_xphone.csv\",\n",
    "    # \"/workspace/metadata/filelists/xphoneBERT/jp_one_piece_xphone.csv\",\n",
    "    # \"/workspace/metadata/filelists/xphoneBERT/jp_skyrim_xphone.csv\",\n",
    "    # \"/workspace/dataset/fr/Fallout4/fr_fallout4_xphone.csv\",\n",
    "    \"/workspace/dataset/de/Fallout4/de_fallout4_xphone.csv\",\n",
    "    \"/workspace/dataset/en/Fallout4/en_fallout4_xphone.csv\",\n",
    "]\n",
    "\n",
    "all_lines = []\n",
    "\n",
    "for file in train_all:\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        all_lines.extend(lines)\n",
    "\n",
    "random.shuffle(all_lines)\n",
    "\n",
    "files_max_per_speaker = 50\n",
    "min_audio_length = 0.3 * 22050\n",
    "max_audio_length = 12.0 * 22050\n",
    "\n",
    "speaker_files_dict = {}\n",
    "\n",
    "with open(\"/workspace/tts_train_slim.csv\", \"w\") as wf:\n",
    "    for line in all_lines:\n",
    "        cols = line.split(\"|\")\n",
    "        filename = cols[0]\n",
    "        speaker = cols[1]\n",
    "        language = cols[2]\n",
    "        text_orig = cols[3]\n",
    "\n",
    "        filename = filename.replace(\"/mnt/datasets/TTS_Data\", \"/workspace/dataset\")\n",
    "\n",
    "        if any(\n",
    "            v in text_orig\n",
    "            for v in [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \"v10\"]\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        if not Path(filename).exists():\n",
    "            continue\n",
    "\n",
    "        if max_audio_length < Path(filename).stat().st_size // 2 < min_audio_length:\n",
    "            continue\n",
    "\n",
    "        if any(char in \"#[]{}*\" for char in text_orig):\n",
    "            continue\n",
    "\n",
    "        if speaker not in speaker_files_dict:\n",
    "            speaker_files_dict[speaker] = []\n",
    "            speaker_files_dict[speaker].append(line)\n",
    "            wf.write(f\"{filename}|{speaker}|{language}|{text_orig}\")\n",
    "        else:\n",
    "            if len(speaker_files_dict[speaker]) < files_max_per_speaker:\n",
    "                speaker_files_dict[speaker].append(line)\n",
    "                wf.write(f\"{filename}|{speaker}|{language}|{text_orig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppgs\n",
    "import random\n",
    "from glob import glob\n",
    "import torch\n",
    "import utils\n",
    "from modules.mel_processing import mel_spectrogram_torch\n",
    "import torchaudio\n",
    "import utils\n",
    "from modules.commons import dedup_seq\n",
    "\n",
    "random_wav = random.choice(\n",
    "    glob(\"/workspace/dataset/de/GameTTS/**/*.wav\", recursive=True)\n",
    ")\n",
    "# Load speech audio at correct sample rate\n",
    "audio = ppgs.load.audio(random_wav)\n",
    "\n",
    "# Choose a gpu index to use for inference. Set to None to use cpu.\n",
    "gpu = None\n",
    "\n",
    "# Infer PPGs\n",
    "ppg = ppgs.from_audio(audio, ppgs.SAMPLE_RATE, gpu=gpu).float()\n",
    "\n",
    "# get mel spectrogram\n",
    "audio, sr = torchaudio.load(random_wav)\n",
    "mel = mel_spectrogram_torch(audio, 1024, 128, 22050, 256, 1024, 0, 8000)\n",
    "\n",
    "print(ppg.shape, mel.shape)\n",
    "\n",
    "ppg = utils.repeat_expand_2d(ppg.squeeze(0), mel.shape[-1], mode=\"nearest\")\n",
    "\n",
    "print(ppg.shape)\n",
    "\n",
    "sparse_ppg = ppgs.sparsify(ppg=ppg, method=\"percentile\", threshold=torch.Tensor([0.85]))\n",
    "most_probable_ppg = torch.argmax(sparse_ppg, dim=1)\n",
    "features, feature_dur = dedup_seq(most_probable_ppg)\n",
    "print(features, feature_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from modules.mel_processing import mel_spectrogram_torch\n",
    "import torchaudio\n",
    "import utils\n",
    "\n",
    "audio, sr = torchaudio.load(random_wav)\n",
    "mel = mel_spectrogram_torch(audio, 1024, 80, 22050, 256, 1024, 0, 8000)\n",
    "print(mel.shape)\n",
    "utils.plot_spectrogram_to_numpy(mel[0].cpu().numpy(), return_figure=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.mel_processing import mel_spectrogram_torch\n",
    "import torchaudio\n",
    "import utils\n",
    "from glob import glob\n",
    "import random\n",
    "import torch\n",
    "from modules.commons import average_over_durations\n",
    "\n",
    "random_ppg_unit = random.choice(\n",
    "    glob(\"/workspace/dataset/de/GameTTS/**/*.ppg_unit.pt\", recursive=True)\n",
    ")\n",
    "ppg_unit = torch.load(random_ppg_unit)\n",
    "\n",
    "wav_path = random_ppg_unit.replace(\".ppg_unit.pt\", \".wav\")\n",
    "audio, sr = torchaudio.load(wav_path)\n",
    "mel = mel_spectrogram_torch(audio, 1024, 128, 22050, 256, 1024, 0, 8000)\n",
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_dur = ppg_unit[\"ppg_unit_dur\"].unsqueeze(0)\n",
    "average_over_durations(mel, unit_dur).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import pydub\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_ppgs = glob(\"/workspace/dataset/de/GameTTS/**/*.ppg_unit.pt\", recursive=True)\n",
    "all_ppgs = [ppg.replace(\".ppg_unit.pt\", \".wav\") for ppg in all_ppgs]\n",
    "files = \"/workspace/vc_train.csv\"\n",
    "\n",
    "speaker_files = defaultdict(list)\n",
    "\n",
    "with open(files, \"r\") as f:\n",
    "    for line in f:\n",
    "        cols = line.split(\"|\")\n",
    "        path = cols[0]\n",
    "        if path in all_ppgs:\n",
    "            audio = pydub.AudioSegment.from_file(path)\n",
    "            duration = audio.duration_seconds\n",
    "\n",
    "            if 10 > duration > 0.3:\n",
    "                speaker = cols[1]\n",
    "                speaker_files[speaker].append(path)\n",
    "                all_ppgs.remove(path)\n",
    "\n",
    "speaker_idx = 0\n",
    "\n",
    "with open(\"/workspace/vc_train_ppg.csv\", \"w\") as wf:\n",
    "    for speaker, files in speaker_files.items():\n",
    "        if len(files) > 5:\n",
    "            for file in files:\n",
    "                wf.write(f\"{file}|{speaker_idx}\\n\")\n",
    "            speaker_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub\n",
    "from tqdm import tqdm\n",
    "\n",
    "files = [\"/workspace/tts_test_ph.csv\", \"/workspace/tts_train_ph_cleaned.csv\"]\n",
    "\n",
    "# /workspace/dataset/pl/Archolos/dia_captain_archolos_q305_amulet_03_06.wav|Captain|2|zaras, zaras! mɔʐɛ i ɲɛ dawɛm vam ʂalup, alɛ na arxɔlɔs vas dɔstart͡ʂɨwɛm. ɲɛ mamɨ dɔ t͡ʂɛɡɔ vrat͡sat͡ɕ.|Zaraz, zaraz! Może i nie dałem wam szalup, ale na Archolos was dostarczyłem. Nie mamy do czego wracać.\n",
    "\n",
    "\n",
    "speaker_id = 0\n",
    "speaker_id_map = {}\n",
    "\n",
    "for f in files:\n",
    "    with open(f, \"r\") as rf:\n",
    "        lines = rf.readlines()\n",
    "\n",
    "    with open(f.replace(\"_ph\", \"_dur_ph\"), \"w\") as wf:\n",
    "        for line in tqdm(lines):\n",
    "            cols = line.split(\"|\")\n",
    "            filename = cols[0]\n",
    "            speaker_name = cols[1]\n",
    "            language = cols[2]\n",
    "            phoneme_text = cols[3]\n",
    "            text = cols[4].replace(\"\\n\", \"\")\n",
    "\n",
    "            audio = pydub.AudioSegment.from_file(filename)\n",
    "            duration = audio.duration_seconds\n",
    "            if 11.0 >= duration >= 0.3 and len(text) > 2:\n",
    "                cols.extend([str(duration)])\n",
    "                wf.write(\n",
    "                    f\"{filename}|{speaker_name}|{language}|{phoneme_text}|{text}|{duration}\\n\"\n",
    "                )\n",
    "\n",
    "                if speaker_name not in speaker_id_map:\n",
    "                    speaker_id_map[speaker_name] = speaker_id\n",
    "                    speaker_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "files = \"/workspace/vc_train.csv\"\n",
    "\n",
    "\n",
    "speaker_files = defaultdict(list)\n",
    "\n",
    "with open(files, \"r\") as f:\n",
    "    for line in f:\n",
    "        cols = line.split(\"|\")\n",
    "        path = cols[0]\n",
    "        speaker = cols[1]\n",
    "        speaker_files[speaker].append(path)\n",
    "\n",
    "speaker_idx = 0\n",
    "\n",
    "with open(\"/workspace/vc_train_cleaned.csv\", \"w\") as wf:\n",
    "    for speaker, files in speaker_files.items():\n",
    "        if len(files) > 5:\n",
    "            for file in files:\n",
    "                wf.write(f\"{file}|{speaker_idx}\\n\")\n",
    "            speaker_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
